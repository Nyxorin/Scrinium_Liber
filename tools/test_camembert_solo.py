import os
# Disable parallelism to be safe
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"

from transformers import pipeline

def test_camembert():
    print("üèóÔ∏è Chargement de CamemBERT NER (Jean-Baptiste/camembert-ner)...")
    try:
        # On force le CPU et le tokenizer lent (Python) pour √©viter le crash Rust/Mutex
        print("üîß Tentative avec use_fast=False...")
        from transformers import AutoTokenizer, AutoModelForTokenClassification
        
        tokenizer = AutoTokenizer.from_pretrained("Jean-Baptiste/camembert-ner", use_fast=False)
        model = AutoModelForTokenClassification.from_pretrained("Jean-Baptiste/camembert-ner")
        
        nlp = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple", device=-1)
        print("‚úÖ Mod√®le charg√© (Slow Tokenizer).")
        
        text = "Malko Linge a rencontr√© Abdi en Somalie pr√®s du Jubba."
        print(f"üìù Analyse de : '{text}'")
        
        results = nlp(text)
        print("üîç R√©sultats :")
        for ent in results:
            print(f"   - {ent['entity_group']}: {ent['word']} (Score: {ent['score']:.2f})")
            
    except Exception as e:
        print(f"‚ùå Erreur : {e}")

if __name__ == "__main__":
    test_camembert()
